[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Joel Parker Blog",
    "section": "",
    "text": "Causal Inference in Statistics: A Primer. Preliminaries\n\n\n\n\n\n\n\nStatistics\n\n\nCausal-Inference\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2023\n\n\nJoel Parker\n\n\n\n\n\n\n  \n\n\n\n\nThe Step From Mixed Models to Generalized Least Squares\n\n\n\n\n\n\n\nStatistics\n\n\nTutorial\n\n\n\n\n\n\n\n\n\n\n\nAug 5, 2023\n\n\nJoel Parker\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nBioinformatics\n\n\nStatistics\n\n\nOpinion\n\n\nTutorial\n\n\n\n\n\n\n\n\n\n\n\nAug 5, 2023\n\n\nJoel Parker\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Variance-Covariance Decomposition/index.html",
    "href": "posts/Variance-Covariance Decomposition/index.html",
    "title": "Causal Inference in Statistics: A Primer. Preliminaries",
    "section": "",
    "text": "A few months ago an advisor of mine mentioned some of the brilliant work that was being done in the of causal inference. This was not the first time I had heard about causal inference and it was probably closer to the 25th, but as a PhD student with a full course load and a job, it was hard to find the time to dive into this area of statistics. Recently, I purchased the Book of Why by Judea Peal. My idea was to read this book as a form of “light” reading after a long day of class to help introduce me to some of the ideas within causal inference, after all, The Book of Why was written by Peal to introduce these ideas to a general audience. However, after the first chapter I found myself getting excited about the topic and wanting to learn more. I decided I needed to find a reference that was more intended for someone like me. Specifically, I wanted to see some formulas. I don’t know how I got to be this way but here I am, and a book, filled with text, filled with jargon, is now my preferred method of learning. Congratulations academia, the transformation is complete.\nFast forward to now. I have completed my course work, working part time, and I am waiting for may adviser to get back leave so I can complete my comps. Please, don’t get me wrong, I still have PLENTY of work to do like writing my proposal, writing papers and studying for comps. However, I do believe that taking some time to expand my knowledge about an area of a field which I am getting a PhD in, is time well spent and who knows, maybe this could turn into part of my dissertation. Sounds like a win-win.\nThe first book I will be reading is “Causal inference in statistics: a primer” by Judea Pearl, Madelyn Glymour, and Nicholas Jewell. This blog with serve as a way to me to take notes and refine my understanding about the topics discussed in this book."
  },
  {
    "objectID": "posts/Variance-Covariance Decomposition/index.html#introduction",
    "href": "posts/Variance-Covariance Decomposition/index.html#introduction",
    "title": "Causal Inference in Statistics: A Primer. Preliminaries",
    "section": "",
    "text": "A few months ago an advisor of mine mentioned some of the brilliant work that was being done in the of causal inference. This was not the first time I had heard about causal inference and it was probably closer to the 25th, but as a PhD student with a full course load and a job, it was hard to find the time to dive into this area of statistics. Recently, I purchased the Book of Why by Judea Peal. My idea was to read this book as a form of “light” reading after a long day of class to help introduce me to some of the ideas within causal inference, after all, The Book of Why was written by Peal to introduce these ideas to a general audience. However, after the first chapter I found myself getting excited about the topic and wanting to learn more. I decided I needed to find a reference that was more intended for someone like me. Specifically, I wanted to see some formulas. I don’t know how I got to be this way but here I am, and a book, filled with text, filled with jargon, is now my preferred method of learning. Congratulations academia, the transformation is complete.\nFast forward to now. I have completed my course work, working part time, and I am waiting for may adviser to get back leave so I can complete my comps. Please, don’t get me wrong, I still have PLENTY of work to do like writing my proposal, writing papers and studying for comps. However, I do believe that taking some time to expand my knowledge about an area of a field which I am getting a PhD in, is time well spent and who knows, maybe this could turn into part of my dissertation. Sounds like a win-win.\nThe first book I will be reading is “Causal inference in statistics: a primer” by Judea Pearl, Madelyn Glymour, and Nicholas Jewell. This blog with serve as a way to me to take notes and refine my understanding about the topics discussed in this book."
  },
  {
    "objectID": "posts/Variance-Covariance Decomposition/index.html#why-study-causation",
    "href": "posts/Variance-Covariance Decomposition/index.html#why-study-causation",
    "title": "Causal Inference in Statistics: A Primer. Preliminaries",
    "section": "Why Study Causation?",
    "text": "Why Study Causation?\nOne of the questions tackled first section of this book is, what can the concept of causation, considered on it own, tell us about the world that tried and true statistical methods can not? If causation is well defined on it’s own, then we can use causation as an addition/addendum to traditional statistics. When used together, causal inference can uncover the workings of the world that traditional methods can not."
  },
  {
    "objectID": "posts/Variance-Covariance Decomposition/index.html#simpsons-paradox",
    "href": "posts/Variance-Covariance Decomposition/index.html#simpsons-paradox",
    "title": "Causal Inference in Statistics: A Primer. Preliminaries",
    "section": "Simpsons Paradox",
    "text": "Simpsons Paradox\nSimpsons paradox is discussed to introduce a motivation for causal inference. Statisticians are familiar with this paradox and Simpsons happens to be one of my favorite paradoxes to bring up with non-statistical friends. The idea that something can be good for men, good for woman and bad for the population, is allays mind boggling to people and to my myself. However, the authors did not just pull from a bag of clever examples just to demonstrate the importance of stratifying to help research make decisions about treatment effects. They demonstrated two examples which had opposite conclusions, despite having the same values after stratification.\nConsider example 1, where recovery rates were recorded of 700 patients after taking a drug. A total of 350 patients took the drug and 350 patients did not and the gender of each person was also recorded.\n\nExample 1\n\n\n\n\n\n\n\n\nDrug\nNo Drug\n\n\n\n\nMen\n81 out of 87 recovered (93%)\n234 out of 270 recovered (87%)\n\n\nWoman\n192 out of 263 recovered (73%)\n55 out of 80 recovered (69%)\n\n\nTotal\n273 out of 350 recovered (78%)\n289 out of 350 recovered (83%)\n\n\n\nWhen we look at the data in example 1, for both men and woman, people who took the drug showed higher recovery rates. However, when you remove the gender information and look at the population recovery rates, people who DID NOT take the drug showed higher recovery rates.\nIn this example, how do we make conclusions about whether or not a person should take the drug? Should we base our conclusions on the overall population rate? Or, should we base our conclusions off of the men/woman sub-populations?\nThe authors argue that the answers to these questions can not be found in the data alone. Instead, we must consider the causal mechanism that generated the results of the data. Suppose, that previous studies showed that estrogen negatively impacted recovery rates and that woman are significantly more likely to take the drug than men. Then we can say that being a woman is a common cause of taking the drug and failing to recover. Therefore, we need to compare recovery rates within gender and we should utilize the recovery rates from the men/woman sub populations.\nNow consider example 2, the results have the same recovery rates (with drug and no drug switched), but instead or recording gender, post-treatment blood pressure was recorded. Now would we want to use the overall recovery rates, or the stratified recovery rates? What if we knew that the effects recovery rates by lowering blood pressure? Since BP is one of the mechanisms that the drug effects treatment, we would want to use the general population rates.\n\nExample 2\n\n\n\n\n\n\n\n\nNo Drug\nDrug\n\n\n\n\nLow BP\n81 out of 87 recovered (93%)\n234 out of 270 recovered (87%)\n\n\nHigh BP\n192 out of 263 recovered (73%)\n55 out of 80 recovered (69%)\n\n\nTotal\n273 out of 350 recovered (78%)\n289 out of 350 recovered (83%)\n\n\n\nThe purpose of these two examples is to show that the decision to treat was not based on the data alone and required that we utilized information outside of the data. Statistics alone can not uncover the causal story. However, causal inference encompasses a set to tools that can be used to express and interpret causal assumptions. When used as an aid to statistics, we can uncover causal relationships."
  },
  {
    "objectID": "posts/Variance-Covariance Decomposition/index.html#structural-causal-models",
    "href": "posts/Variance-Covariance Decomposition/index.html#structural-causal-models",
    "title": "Causal Inference in Statistics: A Primer. Preliminaries",
    "section": "Structural Causal Models",
    "text": "Structural Causal Models\nThe authors give some ideas about basic probability theory before moving on to the introduction of graphs and structural causal models. However, I am going to skip over the introduction to probablity and focus on graphs and SCM.\n\nGraphs\nWhen talking about graphs it is easy to think about a collection of visualization tools which aid in our understanding of trends and patterns. However, in mathematics, graphs have a formal definition. A graph consists of two elements:\n\nNodes - Sometimes called vertices, are often represented as dots or circles.\nEdges - Are lines or arrows connecting the dot/circles.\n\nAn example of nodes and edges may be simlar to what is displayed below. The edges of the graph can be directed (with arrow) or undirected (without and arrow).\n\n\n\n\ngraph LR;\n    A((A))---B((B))\n\n\n\n\n\n\n\n\n\ngraph LR;\n    C((C))--&gt;D((D))\n\n\n\n\n\nWhen we have a directed graph and their exists a path from a node to a itself, this is referenced to as a cyclic graph.\n\n\n\n\ngraph LR;\n    A((A)) --&gt; B((B))\n    B --&gt; C((C))\n    C --&gt; A\n\n\n\n\n\n\nIf no such path exists then this is called an acyclic graph.\n\n\n\n\ngraph LR;\n    A((A)) --&gt; B((B))\n    B --&gt; C((C))\n    A --&gt; C"
  },
  {
    "objectID": "posts/Generalized Least Squares/index.html",
    "href": "posts/Generalized Least Squares/index.html",
    "title": "The Step From Mixed Models to Generalized Least Squares",
    "section": "",
    "text": "We often used mixed effect models in practice when dealing with repeated measures types of experiments or clustered data. However, one of the draw backs of mixed models is that they assume the within-group errors \\(e_i\\) are heteroskedastic (equal variance) and uncorrelated. There are many applications where this is not the case, and the within group error are not heteroskedastic or are correlated. One example of this is in a longitudinal analysis where we are collecting repeated measures from an individual over time. One method for modeling this type of data is to use a random intercept model which is going to assume that the within subject error is independent and heteroskedastics. In other words, we are assuming that the observations from the same person are interchangeable. The issue arises when thinking about the nature of longitudinal data, such that measurements from the same subject are going to be more correlated if they were measured closer in time than measurements that were measured farther apart in time. This induces a correlation structure to the data that is not accounted for with the standard random intercept model. Frank Harrell goes into detail about the compound symmetric correlation structure induced by the random intercept models and why this is not an appropriate way to model longitudinal data in many cases, this article can be found here.\nFor this article we will discussing how the basic mixed effects model can be extended to allow for herteroskedastic and correlated within-group errors. The details of this post are as described in Mixed-Effects Models in S and S-Plus (Pinheiro and Bates 2006), chapter 5."
  },
  {
    "objectID": "posts/Generalized Least Squares/index.html#issues-with-basic-mixed-models",
    "href": "posts/Generalized Least Squares/index.html#issues-with-basic-mixed-models",
    "title": "The Step From Mixed Models to Generalized Least Squares",
    "section": "",
    "text": "We often used mixed effect models in practice when dealing with repeated measures types of experiments or clustered data. However, one of the draw backs of mixed models is that they assume the within-group errors \\(e_i\\) are heteroskedastic (equal variance) and uncorrelated. There are many applications where this is not the case, and the within group error are not heteroskedastic or are correlated. One example of this is in a longitudinal analysis where we are collecting repeated measures from an individual over time. One method for modeling this type of data is to use a random intercept model which is going to assume that the within subject error is independent and heteroskedastics. In other words, we are assuming that the observations from the same person are interchangeable. The issue arises when thinking about the nature of longitudinal data, such that measurements from the same subject are going to be more correlated if they were measured closer in time than measurements that were measured farther apart in time. This induces a correlation structure to the data that is not accounted for with the standard random intercept model. Frank Harrell goes into detail about the compound symmetric correlation structure induced by the random intercept models and why this is not an appropriate way to model longitudinal data in many cases, this article can be found here.\nFor this article we will discussing how the basic mixed effects model can be extended to allow for herteroskedastic and correlated within-group errors. The details of this post are as described in Mixed-Effects Models in S and S-Plus (Pinheiro and Bates 2006), chapter 5."
  },
  {
    "objectID": "posts/Generalized Least Squares/index.html#mixed-effects-model",
    "href": "posts/Generalized Least Squares/index.html#mixed-effects-model",
    "title": "The Step From Mixed Models to Generalized Least Squares",
    "section": "Mixed Effects Model",
    "text": "Mixed Effects Model\nTo see how we can extend mixed effects models, we will first show the mixed model formulation for a single level. Below is a table of symbols used in the modeling that can be refereed to throughout this blog.\n\n\n\n\n\n\n\n\nSymbol\nDimensions\nRepresentation\n\n\n\n\n\\(y\\)\n\nContinuous Outcome\n\n\n\\(i\\)\n\nGroup (Cluster) number\n\n\n\\(n_i\\)\n\nNumber of observations in the \\(i^{th}\\) group\n\n\n\\(\\mathbf{y}_i\\)\n\\(n_i \\times 1\\)\nOutcome vector for the \\(i^{th}\\) group\n\n\n\\(M\\)\n\nnumber of groups\n\n\n\\(p\\)\n\nnumber of fixed effects\n\n\n\\(\\mathbf{b}\\)\n\\(q\\times 1\\)\nmatrix of random effects\n\n\n\\(\\mathbf{Z_i}\\)\n\\(n_i \\times q\\)\nRandom effects regression matrix\n\n\n\\(\\mathbf{X}_i\\)\n\\(n_i \\times p\\)\nFixed effects regression matrix\n\n\n\\(\\beta\\)\n\\(p \\times 1\\)\nVector of fixed effects\n\n\n\\(\\Lambda\\)\n\\(n_i \\times n_i\\)\nCovariance matrix for within group errors\n\n\n\nAssume we have a continuous outcome variable \\(y\\). All \\(n_i\\) observations from the \\(i^{th}\\) group are expressed in an \\(n_i\\) dimensional vector \\(\\mathbf{Y}_i\\). We can model \\(\\mathbf{y}_i\\) as:\n\\[\n\\begin{aligned}\n\\mathbf{Y_i}_{n_i \\times 1} = \\mathbf{X}_i\\beta + \\mathbf{Z}_i\\mathbf{b}_i + \\epsilon_i,  i=1 , \\dots, M \\\\\n\\mathbf{b}_i \\sim \\mathbf{N}(0, \\Sigma_{q\\times q}), \\ \\epsilon_i \\sim \\mathcal{N}(\\mathbf{0},\\sigma^2 \\mathbf{I}_{n_i \\times n_i})\n\\end{aligned}\n\\] Where \\(\\beta\\) is a \\(p\\) dimensional vector of fixed effects, \\(\\mathbf{X}_i\\) is a \\(n_i \\times p\\) fixed effects matrix, \\(\\mathbf{Z_i}\\) is a \\(n_i \\times q\\) random effects matrix and \\(\\mathbf{b}\\) is a vector of random effects.\nThere are two distributional assumptions made for this model, the first is that the random effects follow a multi-variate normal distribution centered at \\(\\mathbf{0}\\) with a covariance matrix \\(\\Sigma_{q \\times q}\\). The second assumption is on the model errors \\(\\epsilon_i\\), which is the within group error matrix, which follows a normal distribution centered at \\(\\mathbf{0}\\) with \\(Var(\\epsilon_i) = \\sigma^2 \\mathbf{I}\\). We therefore, are placing the assumption that the within group errors are independent and identically distributed. Furthermore, this assumption may not hold for many correlation structures in practice, like longitudinal data analysis.\nPinheiro and Bates outline how to fit this model in chapter 2 of (Pinheiro and Bates 2006). I will not go into detail here, but the methods used to fit the model above will be use full for fitting the generalized least squares model defined in the next section."
  },
  {
    "objectID": "posts/Generalized Least Squares/index.html#relaxing-assumptions-of-mixed-models.",
    "href": "posts/Generalized Least Squares/index.html#relaxing-assumptions-of-mixed-models.",
    "title": "The Step From Mixed Models to Generalized Least Squares",
    "section": "Relaxing Assumptions of mixed models.",
    "text": "Relaxing Assumptions of mixed models.\nAs discussed the previous section, the assumptions placed within group error \\(\\epsilon_i\\) may may be violated for many applications, and we may need to relax this assumption. Here, we define an extended single-level mixed effects model that relaxes this assumption by allowing heteroskedastic and within group errors.\n\\[\n\\begin{aligned}\n\\mathbf{Y_i}_{n_i \\times 1} = \\mathbf{X}_i\\beta + \\mathbf{Z}_i\\mathbf{b}_i + \\epsilon_i,  i=1 , \\dots, M \\\\\n\\mathbf{b}_i \\sim \\mathbf{N}(0, \\Sigma_{q\\times q}), \\ \\epsilon_i \\sim \\mathcal{N}(\\mathbf{0},\\sigma^2\\Lambda_{i: n_i \\times n_i})\n\\end{aligned}\n\\]\nHere, the only difference between the two models is the assumption of covariance matrix for the within group error, \\(\\mathbf{e}_i\\), was changed to a more flexible, covariance matrix \\(\\Lambda_i\\) which is parameterized by a small set of parameters \\(\\lambda\\). Similarly, to the first model the \\(\\epsilon_i\\) are assumed to be independent for different \\(i\\) and independent from the random effect \\(\\mathbf{b}_i\\).\nThis extended mixed model can be fit by utilizing the fact that covariance matrices are symmetric and positive semi-definite and therefore \\(\\Lambda\\) has an invertable square root \\(\\Lambda^{-1/2}\\). We can perform a linear transformation of \\(\\mathbf{Y}_i\\), \\(\\mathbf{X}_i\\), \\(\\mathbf{Z}_i\\) and \\(\\epsilon\\) by \\(\\Lambda_i^{-1/2}\\) such that:\n\\[\n\\begin{cases}\n\\mathbf{Y}_i^* = (\\Lambda_i^{-1/2})^T \\mathbf{Y}_i & \\mathbf{X}_i^* = (\\Lambda_i^{-1/2})^T \\mathbf{X}_i \\\\\n\\mathbf{Z}_i^* = (\\Lambda_i^{-1/2})^T\\mathbf{Z}_i & \\epsilon^*_i = (\\Lambda_i^{-1/2})^T \\epsilon_i^*,\n\\end{cases}\n\\]\nAfter this linear transformation it can be shown that \\(\\epsilon^*_i\\sim \\mathcal{N}(\\mathbf{0},\\sigma^2\\mathbf{I})\\). Therefore, we can rewrite the extended single level model as:\n\\[\n\\begin{aligned}\n\\mathbf{Y_i}^*_{n_i \\times 1} = \\mathbf{X}^*_i\\beta + \\mathbf{Z}^*_i\\mathbf{b}_i + \\epsilon^*_i,  i=1 , \\dots, M \\\\\n\\mathbf{b}_i \\sim \\mathbf{N}(0, \\Sigma_{q\\times q}), \\ \\epsilon^*_i \\sim \\mathcal{N}(\\mathbf{0},\\sigma^2 \\mathbf{I}_{n_i \\times n_i}),\n\\end{aligned}\n\\]\nWhich takes the same form as the basic mixed model and can be solved with the same methods discussed in chapter 2 of (Pinheiro and Bates 2006)."
  },
  {
    "objectID": "posts/Generalized Least Squares/index.html#generalized-least-squares",
    "href": "posts/Generalized Least Squares/index.html#generalized-least-squares",
    "title": "The Step From Mixed Models to Generalized Least Squares",
    "section": "Generalized Least Squares",
    "text": "Generalized Least Squares\nOne important thing to note, is that if we are not be interested in the random effects of the model we can choose to not incorporate them into the model. To do this we use the within-group component \\(\\Lambda_i\\) to directly model the variance-covariance matrix as seen below.\n\\[\n\\begin{aligned}\n\\mathbf{Y_i}_{n_i \\times 1} = \\mathbf{X}_i\\beta  + \\epsilon_i,  i=1 , \\dots, M \\\\\n\\ \\epsilon_i \\sim \\mathcal{N}(\\mathbf{0},\\sigma^2\\Lambda_{i: n_i \\times n_i})\n\\end{aligned}\n\\]\nforming the generalized least squares model. Similar to the extended mixed model above, we can transform the model matrices by \\(\\Lambda^{-1/2}\\) leading to a “classic” linear regression problem\n\\[\n\\begin{aligned}\n\\mathbf{Y_i}^*_{n_i \\times 1} = \\mathbf{X}^*_i\\beta + \\epsilon^*_i,  i=1 , \\dots, M \\\\\n\\epsilon^*_i \\sim \\mathcal{N}(\\mathbf{0},\\sigma^2 \\mathbf{I}_{n_i \\times n_i})\n\\end{aligned}\n\\]\nTherefore, the estimators for \\(\\hat{\\beta}\\) and \\(\\hat{\\sigma}^2\\) are found by solving the ordinary least squares problem for given values of \\(\\lambda\\).\n\\[\n\\hat{\\beta}(\\lambda) = [(\\mathbf{X}^*)^T\\mathbf{X}^*]^{-1}(\\mathbf{X}^*)\\mathbf{Y}^* \\\\\n\\hat{\\sigma}^2(\\lambda) = \\frac{\\|\\mathbf{Y}^* - \\mathbf{X}^*\\hat{\\beta}(\\lambda)\\|}{N}\n\\]\nThe optimal values for \\(\\lambda\\) is found by maximizing the profiled log-likelihood as described in (Pinheiro and Bates 2006)."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Hello! My name is Joel Parker and I am a Ph.D student in Biostatistics at the University of Arizona. I am developing this blog to provide a place to share my journey through the later years of my Ph.D with a broader audience. In doing this, I hope to accomplish two goals.\n\nShare what I am learning as a Ph.D student in Biostatistics.\nTo grow as a scientist, author and communicator.\n\nAs a Ph.D student I have found great pleasure in reading blogs written by some of my statistical heroes like Andrew Gelman and Frank Harrell, which has partially inspired me to start this blog. Another inspiration is to connect with other young scientists who are at a similar stage in their career as me, because I believe we can learn from each other. Therefore, I welcome feedback and suggestions to what you found useful or if you have found a better way.\nI look forward to connecting with you!\nJoel Parker"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello! My name is Joel Parker and I am a Ph.D student in Biostatistics at the University of Arizona. I am developing this blog to provide a place to share my journey through the later years of my Ph.D with a broader audience. In doing this, I hope to accomplish two goals.\n\nShare what I am learning as a Ph.D student in Biostatistics.\nTo grow as a scientist, author and communicator.\n\nAs a Ph.D student I have found great pleasure in reading blogs written by some of my statistical heroes like Andrew Gelman and Frank Harrell, which has partially inspired me to start this blog. Another inspiration is to connect with other young scientists who are at a similar stage in their career as me, because I believe we can learn from each other. Therefore, I welcome feedback and suggestions to what you found useful or if you have found a better way.\nI look forward to connecting with you!\nJoel Parker"
  }
]