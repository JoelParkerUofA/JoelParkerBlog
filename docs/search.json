[
  {
    "objectID": "posts/Generalized Least Squares/index.html",
    "href": "posts/Generalized Least Squares/index.html",
    "title": "The Step From Mixed Models to Generalized Least Squares",
    "section": "",
    "text": "We often used mixed effect models in practice when dealing with repeated measures types of experiments or clustered data. However, one of the draw backs of mixed models is that they assume the within-group errors \\(e_i\\) are heteroskedastic (equal variance) and uncorrelated. There are many applications where this is not the case, and the within group error are not heteroskedastic or are correlated. One example of this is in a longitudinal analysis where we are collecting repeated measures from an individual over time. One method for modeling this type of data is to use a random intercept model which is going to assume that the within subject error is independent and heteroskedastics. In other words, we are assuming that the observations from the same person are interchangeable. The issue arises when thinking about the nature of longitudinal data, such that measurements from the same subject are going to be more correlated if they were measured closer in time than measurements that were measured farther apart in time. This induces a correlation structure to the data that is not accounted for with the standard random intercept model. Frank Harrell goes into detail about the compound symmetric correlation structure induced by the random intercept models and why this is not an appropriate way to model longitudinal data in many cases, this article can be found here.\nFor this article we will discussing how the basic mixed effects model can be extended to allow for herteroskedastic and correlated within-group errors. The details of this post are as described in Mixed-Effects Models in S and S-Plus (Pinheiro and Bates 2006), chapter 5."
  },
  {
    "objectID": "posts/Generalized Least Squares/index.html#issues-with-basic-mixed-models",
    "href": "posts/Generalized Least Squares/index.html#issues-with-basic-mixed-models",
    "title": "The Step From Mixed Models to Generalized Least Squares",
    "section": "",
    "text": "We often used mixed effect models in practice when dealing with repeated measures types of experiments or clustered data. However, one of the draw backs of mixed models is that they assume the within-group errors \\(e_i\\) are heteroskedastic (equal variance) and uncorrelated. There are many applications where this is not the case, and the within group error are not heteroskedastic or are correlated. One example of this is in a longitudinal analysis where we are collecting repeated measures from an individual over time. One method for modeling this type of data is to use a random intercept model which is going to assume that the within subject error is independent and heteroskedastics. In other words, we are assuming that the observations from the same person are interchangeable. The issue arises when thinking about the nature of longitudinal data, such that measurements from the same subject are going to be more correlated if they were measured closer in time than measurements that were measured farther apart in time. This induces a correlation structure to the data that is not accounted for with the standard random intercept model. Frank Harrell goes into detail about the compound symmetric correlation structure induced by the random intercept models and why this is not an appropriate way to model longitudinal data in many cases, this article can be found here.\nFor this article we will discussing how the basic mixed effects model can be extended to allow for herteroskedastic and correlated within-group errors. The details of this post are as described in Mixed-Effects Models in S and S-Plus (Pinheiro and Bates 2006), chapter 5."
  },
  {
    "objectID": "posts/Generalized Least Squares/index.html#mixed-effects-model",
    "href": "posts/Generalized Least Squares/index.html#mixed-effects-model",
    "title": "The Step From Mixed Models to Generalized Least Squares",
    "section": "Mixed Effects Model",
    "text": "Mixed Effects Model\nTo see how we can extend mixed effects models, we will first show the mixed model formulation for a single level. Below is a table of symbols used in the modeling that can be refereed to throughout this blog.\n\n\n\n\n\n\n\n\nSymbol\nDimensions\nRepresentation\n\n\n\n\n\\(y\\)\n\nContinuous Outcome\n\n\n\\(i\\)\n\nGroup (Cluster) number\n\n\n\\(n_i\\)\n\nNumber of observations in the \\(i^{th}\\) group\n\n\n\\(\\mathbf{y}_i\\)\n\\(n_i \\times 1\\)\nOutcome vector for the \\(i^{th}\\) group\n\n\n\\(M\\)\n\nnumber of groups\n\n\n\\(p\\)\n\nnumber of fixed effects\n\n\n\\(\\mathbf{b}\\)\n\\(q\\times 1\\)\nmatrix of random effects\n\n\n\\(\\mathbf{Z_i}\\)\n\\(n_i \\times q\\)\nRandom effects regression matrix\n\n\n\\(\\mathbf{X}_i\\)\n\\(n_i \\times p\\)\nFixed effects regression matrix\n\n\n\\(\\beta\\)\n\\(p \\times 1\\)\nVector of fixed effects\n\n\n\\(\\Lambda\\)\n\\(n_i \\times n_i\\)\nCovariance matrix for within group errors\n\n\n\nAssume we have a continuous outcome variable \\(y\\). All \\(n_i\\) observations from the \\(i^{th}\\) group are expressed in an \\(n_i\\) dimensional vector \\(\\mathbf{Y}_i\\). We can model \\(\\mathbf{y}_i\\) as:\n\\[\n\\mathbf{Y_i}_{n_i \\times 1} = \\mathbf{X}_i\\beta + \\mathbf{Z}_i\\mathbf{b}_i + \\epsilon_i,  i=1 , \\dots, M \\\\\n\\mathbf{b}_i \\sim \\mathbf{N}(0, \\Sigma_{q\\times q}), \\ \\epsilon_i \\sim \\mathcal{N}(\\mathbf{0},\\sigma^2 \\mathbf{I}_{n_i \\times n_i})\n\\] Where \\(\\beta\\) is a \\(p\\) dimensional vector of fixed effects, \\(\\mathbf{X}_i\\) is a \\(n_i \\times p\\) fixed effects matrix, \\(\\mathbf{Z_i}\\) is a \\(n_i \\times q\\) random effects matrix and \\(\\mathbf{b}\\) is a vector of random effects.\nThere are two distributional assumptions made for this model, the first is that the random effects follow a multi-variate normal distribution centered at \\(\\mathbf{0}\\) with a covariance matrix \\(\\Sigma_{q \\times q}\\). The second assumption is on the model errors \\(\\epsilon_i\\), which is the within group error matrix, which follows a normal distribution centered at \\(\\mathbf{0}\\) with \\(Var(\\epsilon_i) = \\sigma^2 \\mathbf{I}\\). We therefore, are placing the assumption that the within group errors are independent and identically distributed. Furthermore, this assumption may not hold for many correlation structures in practice, like longitudinal data analysis.\nPinheiro and Bates outline how to fit this model in chapter 2 of (Pinheiro and Bates 2006). I will not go into detail here, but the methods used to fit the model above will be use full for fitting the generalized least squares model defined in the next section."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Hello! My name is Joel Parker and I am a Ph.D student in Biostatistics at the University of Arizona. I am developing this blog to provide a place to share my journey through the later years of my Ph.D with a broader audience. In doing this, I hope to accomplish two goals.\n\nShare what I am learning as a Ph.D student in Biostatistics.\nTo grow as a scientist, author and communicator.\n\nAs a Ph.D student I have found great pleasure in reading blogs written by some of my statistical heroes like Andrew Gelman and Frank Harrell, which has partially inspired me to start this blog. Another inspiration is to connect with other young scientists who are at a similar stage in their career as me, because I believe we can learn from each other. Therefore, I welcome feedback and suggestions to what you found useful or if you have found a better way.\nI look forward to connecting with you!\nJoel Parker"
  },
  {
    "objectID": "Readme.html",
    "href": "Readme.html",
    "title": "Joel Parker's Blog",
    "section": "",
    "text": "This github repository is for Joel Parker’s blog https://joelparkeruofa.github.io/JoelParkerBlog/"
  },
  {
    "objectID": "posts/Generalized Least Squares/index.html#relaxing-assumptions-of-mixed-models.",
    "href": "posts/Generalized Least Squares/index.html#relaxing-assumptions-of-mixed-models.",
    "title": "The Step From Mixed Models to Generalized Least Squares",
    "section": "Relaxing Assumptions of mixed models.",
    "text": "Relaxing Assumptions of mixed models.\nAs discussed the previous section, the assumptions placed within group error \\(\\epsilon_i\\) may may be violated for many applications, and we may need to relax this assumption. Here, we define an extended single-level mixed effects model that relaxes this assumption by allowing heteroskedastic and within group errors.\n\\[\n\\begin{aligned}\n\\mathbf{Y_i}_{n_i \\times 1} = \\mathbf{X}_i\\beta + \\mathbf{Z}_i\\mathbf{b}_i + \\epsilon_i,  i=1 , \\dots, M \\\\\n\\mathbf{b}_i \\sim \\mathbf{N}(0, \\Sigma_{q\\times q}), \\ \\epsilon_i \\sim \\mathcal{N}(\\mathbf{0},\\sigma^2\\Lambda_{i: n_i \\times n_i})\n\\end{aligned}\n\\]\nHere, the only difference between the two models is the assumption of covariance matrix for the within group error, \\(\\mathbf{e}_i\\), was changed to a more flexible, covariance matrix \\(\\Lambda_i\\) which is parameterized by a small set of parameters \\(\\lambda\\). Similarly, to the first model the \\(\\epsilon_i\\) are assumed to be independent for different \\(i\\) and independent from the random effect \\(\\mathbf{b}_i\\).\nThis extended mixed model can be fit by utilizing the fact that covariance matrices are symmetric and positive semi-definite and therefore \\(\\Lambda\\) has an invertable square root \\(\\Lambda^{-1/2}\\). We can perform a linear transformation of \\(\\mathbf{Y}_i\\), \\(\\mathbf{X}_i\\), \\(\\mathbf{Z}_i\\) and \\(\\epsilon\\) by \\(\\Lambda_i^{-1/2}\\) such that:\n\\[\n\\begin{cases}\n\\mathbf{Y}_i^* = (\\Lambda_i^{-1/2})^T \\mathbf{Y}_i & \\mathbf{X}_i^* = (\\Lambda_i^{-1/2})^T \\mathbf{X}_i \\\\\n\\mathbf{Z}_i^* = (\\Lambda_i^{-1/2})^T\\mathbf{Z}_i & \\epsilon^*_i = (\\Lambda_i^{-1/2})^T \\epsilon_i^*,\n\\end{cases}\n\\] After this linear transformation it can be shown that \\(\\epsilon^*_i\\sim \\mathcal{N}(\\mathbf{0},\\sigma^2\\mathbf{I})\\). Therefore, we can rewrite the extended single level model as: \\[\n\\mathbf{Y_i}^*_{n_i \\times 1} = \\mathbf{X}^*_i\\beta + \\mathbf{Z}^*_i\\mathbf{b}_i + \\epsilon^*_i,  i=1 , \\dots, M \\\\\n\\mathbf{b}_i \\sim \\mathbf{N}(0, \\Sigma_{q\\times q}), \\ \\epsilon^*_i \\sim \\mathcal{N}(\\mathbf{0},\\sigma^2 \\mathbf{I}_{n_i \\times n_i}),\n\\]\nWhich takes the same form as the basic mixed model and can be solved with the same methods discussed in chapter 2 of (Pinheiro and Bates 2006)."
  },
  {
    "objectID": "posts/Generalized Least Squares/index.html#generalized-least-squares",
    "href": "posts/Generalized Least Squares/index.html#generalized-least-squares",
    "title": "The Step From Mixed Models to Generalized Least Squares",
    "section": "Generalized Least Squares",
    "text": "Generalized Least Squares\nOne important thing to note, is that if we are not be interested in the random effects of the model we can choose to not incorporate them into the model. To do this we use the within-group component \\(\\Lambda_i\\) to directly model the variance-covariance matrix as seen below.\n\\[\n\\mathbf{Y_i}_{n_i \\times 1} = \\mathbf{X}_i\\beta  + \\epsilon_i,  i=1 , \\dots, M \\\\\n\\ \\epsilon_i \\sim \\mathcal{N}(\\mathbf{0},\\sigma^2\\Lambda_{i: n_i \\times n_i})\n\\]\nforming the generalized least squares model. Similar to the extended mixed model above, we can transform the model matrices by \\(\\Lambda^{-1/2}\\) leading to a “classic” linear regression problem\n\\[\n\\mathbf{Y_i}^*_{n_i \\times 1} = \\mathbf{X}^*_i\\beta + \\epsilon^*_i,  i=1 , \\dots, M \\\\\n\\epsilon^*_i \\sim \\mathcal{N}(\\mathbf{0},\\sigma^2 \\mathbf{I}_{n_i \\times n_i})\n\\] Therefore, the estimators for \\(\\hat{\\beta}\\) and \\(\\hat{\\sigma}^2\\) are found by solving the ordinary least squares problem for given values of \\(\\lambda\\).\n\\[\n\\hat{\\beta}(\\lambda) = [(\\mathbf{X}^*)^T\\mathbf{X}^*]^{-1}(\\mathbf{X}^*)\\mathbf{Y}^* \\\\\n\\hat{\\sigma}^2(\\lambda) = \\frac{\\|\\mathbf{Y}^* - \\mathbf{X}^*\\hat{\\beta}(\\lambda)\\|}{N}\n\\] The optimal value for \\(\\Lambda\\) is found by maximizing the profiled log-likelihood as described in (Pinheiro and Bates 2006)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello! My name is Joel Parker and I am a Ph.D student in Biostatistics at the University of Arizona. I am developing this blog to provide a place to share my journey through the later years of my Ph.D with a broader audience. In doing this, I hope to accomplish two goals.\n\nShare what I am learning as a Ph.D student in Biostatistics.\nTo grow as a scientist, author and communicator.\n\nAs a Ph.D student I have found great pleasure in reading blogs written by some of my statistical heroes like Andrew Gelman and Frank Harrell, which has partially inspired me to start this blog. Another inspiration is to connect with other young scientists who are at a similar stage in their career as me, because I believe we can learn from each other. Therefore, I welcome feedback and suggestions to what you found useful or if you have found a better way.\nI look forward to connecting with you!\nJoel Parker"
  }
]